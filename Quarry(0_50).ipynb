{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quarry(0_50).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7UwuiOcezZt"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import scipy.io\n",
        "import os\n",
        "from numpy.linalg import norm\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy.linalg import det\n",
        "from numpy.linalg import inv\n",
        "from scipy.linalg import rq\n",
        "from numpy.linalg import svd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import sys\n",
        "from scipy import ndimage, spatial\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from skimage import io, transform,data\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "import math\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import sklearn.svm\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from os.path import exists\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.metrics.cluster import completeness_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from functools import partial\n",
        "from torchsummary import summary\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import h5py as h5"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaujCSnyfFdd",
        "outputId": "b2d5567c-60cc-4457-f72d-f0e8b5a1e495"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFqd0mERfQQ-",
        "outputId": "c0c3aa20-a7b1-480f-8d37-990fb7556eb8"
      },
      "source": [
        "!pip install opencv-python==3.4.2.17\n",
        "!pip install opencv-contrib-python==3.4.2.17"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==3.4.2.17) (1.19.5)\n",
            "Requirement already satisfied: opencv-contrib-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s1IumHQkO2c"
      },
      "source": [
        "Dataset='Quarry0.50'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RqfuF2ifc4i"
      },
      "source": [
        "class Image:\n",
        "    def __init__(self, img, position):\n",
        "        \n",
        "        self.img = img\n",
        "        self.position = position\n",
        "\n",
        "inlier_matchset = []\n",
        "def features_matching(a,keypointlength,threshold):\n",
        "  #threshold=0.2\n",
        "  bestmatch=np.empty((keypointlength),dtype= np.int16)\n",
        "  img1index=np.empty((keypointlength),dtype=np.int16)\n",
        "  distance=np.empty((keypointlength))\n",
        "  index=0\n",
        "  for j in range(0,keypointlength):\n",
        "    #For a descriptor fa in Ia, take the two closest descriptors fb1 and fb2 in Ib\n",
        "    x=a[j]\n",
        "    listx=x.tolist()\n",
        "    x.sort()\n",
        "    minval1=x[0]                                # min \n",
        "    minval2=x[1]                                # 2nd min\n",
        "    itemindex1 = listx.index(minval1)           #index of min val    \n",
        "    itemindex2 = listx.index(minval2)           #index of second min value \n",
        "    ratio=minval1/minval2                       #Ratio Test\n",
        "    \n",
        "    if ratio<threshold: \n",
        "      #Low distance ratio: fb1 can be a good match\n",
        "      bestmatch[index]=itemindex1\n",
        "      distance[index]=minval1\n",
        "      img1index[index]=j\n",
        "      index=index+1\n",
        "  return  [cv2.DMatch(img1index[i],bestmatch[i].astype(int),distance[i]) for i in range(0,index)]\n",
        "          \n",
        "   \n",
        "  \n",
        "def compute_Homography(im1_pts,im2_pts):\n",
        "  \"\"\"\n",
        "  im1_pts and im2_pts are 2×n matrices with\n",
        "  4 point correspondences from the two images\n",
        "  \"\"\"\n",
        "  num_matches=len(im1_pts)\n",
        "  num_rows = 2 * num_matches\n",
        "  num_cols = 9\n",
        "  A_matrix_shape = (num_rows,num_cols)\n",
        "  A = np.zeros(A_matrix_shape)\n",
        "  a_index = 0\n",
        "  for i in range(0,num_matches):\n",
        "    (a_x, a_y) = im1_pts[i]\n",
        "    (b_x, b_y) = im2_pts[i]\n",
        "    row1 = [a_x, a_y, 1, 0, 0, 0, -b_x*a_x, -b_x*a_y, -b_x] # First row \n",
        "    row2 = [0, 0, 0, a_x, a_y, 1, -b_y*a_x, -b_y*a_y, -b_y] # Second row \n",
        "\n",
        "    # place the rows in the matrix\n",
        "    A[a_index] = row1\n",
        "    A[a_index+1] = row2\n",
        "\n",
        "    a_index += 2\n",
        "    \n",
        "  U, s, Vt = np.linalg.svd(A)\n",
        "\n",
        "  #s is a 1-D array of singular values sorted in descending order\n",
        "  #U, Vt are unitary matrices\n",
        "  #Rows of Vt are the eigenvectors of A^TA.\n",
        "  #Columns of U are the eigenvectors of AA^T.\n",
        "  H = np.eye(3)\n",
        "  H = Vt[-1].reshape(3,3) # take the last row of the Vt matrix\n",
        "  return H\n",
        "  \n",
        "  \n",
        "def displayplot(img,title):\n",
        "  \n",
        "  plt.figure(figsize=(15,15))\n",
        "  plt.title(title)\n",
        "  plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GOCWqPWfrFu"
      },
      "source": [
        "def get_inliers(f1, f2, matches, H, RANSACthresh):\n",
        "\n",
        "  inlier_indices = []\n",
        "  for i in range(len(matches)):\n",
        "    queryInd = matches[i].queryIdx\n",
        "    trainInd = matches[i].trainIdx\n",
        "\n",
        "    #queryInd = matches[i][0]\n",
        "    #trainInd = matches[i][1]\n",
        "\n",
        "    queryPoint = np.array([f1[queryInd].pt[0],  f1[queryInd].pt[1], 1]).T \n",
        "    trans_query = H.dot(queryPoint) \n",
        "\n",
        "   \n",
        "    comp1 = [trans_query[0]/trans_query[2], trans_query[1]/trans_query[2]] # normalize with respect to z\n",
        "    comp2 = np.array(f2[trainInd].pt)[:2]\n",
        "    \n",
        "\n",
        "    if(np.linalg.norm(comp1-comp2) <= RANSACthresh): # check against threshold\n",
        "      inlier_indices.append(i)\n",
        "  return inlier_indices\n",
        "\n",
        "\n",
        "def RANSAC_alg(f1, f2, matches, nRANSAC, RANSACthresh):\n",
        "\n",
        "      \n",
        "    minMatches = 4\n",
        "    nBest = 0\n",
        "    best_inliers = []\n",
        "    H_estimate = np.eye(3,3)\n",
        "    global inlier_matchset\n",
        "    inlier_matchset=[]\n",
        "    for iteration in range(nRANSAC):\n",
        "      \n",
        "        #Choose a minimal set of feature matches.\n",
        "        matchSample = random.sample(matches, minMatches)\n",
        "        \n",
        "        #Estimate the Homography implied by these matches\n",
        "        im1_pts=np.empty((minMatches,2))\n",
        "        im2_pts=np.empty((minMatches,2))\n",
        "        for i in range(0,minMatches):\n",
        "          m = matchSample[i]\n",
        "          im1_pts[i] = f1[m.queryIdx].pt\n",
        "          im2_pts[i] = f2[m.trainIdx].pt\n",
        "          #im1_pts[i] = f1[m[0]].pt\n",
        "          #im2_pts[i] = f2[m[1]].pt             \n",
        "          \n",
        "        H_estimate=compute_Homography(im1_pts,im2_pts)\n",
        "        \n",
        "               \n",
        "        # Calculate the inliers for the H\n",
        "        inliers = get_inliers(f1, f2, matches, H_estimate, RANSACthresh)\n",
        "\n",
        "        # if the number of inliers is higher than previous iterations, update the best estimates\n",
        "        if len(inliers) > nBest:\n",
        "            nBest= len(inliers)\n",
        "            best_inliers = inliers\n",
        "\n",
        "    print(\"Number of best inliers\",len(best_inliers))\n",
        "    for i in range(len(best_inliers)):\n",
        "      inlier_matchset.append(matches[best_inliers[i]])\n",
        "    \n",
        "    # compute a homography given this set of matches\n",
        "    im1_pts=np.empty((len(best_inliers),2))\n",
        "    im2_pts=np.empty((len(best_inliers),2))\n",
        "    for i in range(0,len(best_inliers)):\n",
        "      m = inlier_matchset[i]\n",
        "      im1_pts[i] = f1[m.queryIdx].pt\n",
        "      im2_pts[i] = f2[m.trainIdx].pt\n",
        "      #im1_pts[i] = f1[m[0]].pt\n",
        "      #im2_pts[i] = f2[m[1]].pt\n",
        "\n",
        "    M=compute_Homography(im1_pts,im2_pts)\n",
        "    return M, best_inliers                    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoGNL8ORfznm"
      },
      "source": [
        "tqdm = partial(tqdm, position=0, leave=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTL3TcsVf2d3"
      },
      "source": [
        "files_all=[]\n",
        "for file in os.listdir(\"/content/drive/My Drive/img\"):\n",
        "    if file.endswith(\".JPG\"):\n",
        "      files_all.append(file)\n",
        "\n",
        "\n",
        "files_all.sort()\n",
        "folder_path = '/content/drive/My Drive/img/'\n",
        "\n",
        "#centre_file = folder_path + files_all[50]\n",
        "left_files_path_rev = []\n",
        "right_files_path = []\n",
        "\n",
        "\n",
        "#Change this according to your dataset split\n",
        "\n",
        "for file in files_all[:int(len(files_all)/2)+1]:\n",
        "  left_files_path_rev.append(folder_path + file)\n",
        "\n",
        "left_files_path = left_files_path_rev[::-1]\n",
        "\n",
        "for file in files_all[int(len(files_all)/2):]:\n",
        "  right_files_path.append(folder_path + file)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaE-aN30gB23",
        "outputId": "c7746d3b-4b19-406c-fd80-e4fcabe09957"
      },
      "source": [
        "import multiprocessing\n",
        "print(multiprocessing.cpu_count())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9RrpYJYgF01",
        "outputId": "48ce522a-6eb4-47e4-bd31-96cdb328ff74"
      },
      "source": [
        "gridsize = 8\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(gridsize,gridsize))\n",
        "\n",
        "images_left_bgr = []\n",
        "images_right_bgr = []\n",
        "\n",
        "images_left = []\n",
        "images_right = []\n",
        "\n",
        "for file in tqdm(left_files_path):\n",
        "  left_image_sat= cv2.imread(file)\n",
        "  lab = cv2.cvtColor(left_image_sat, cv2.COLOR_BGR2LAB)\n",
        "  lab[...,0] = clahe.apply(lab[...,0])\n",
        "  left_image_sat = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "  left_img = cv2.resize(left_image_sat,None,fx=0.50, fy=0.50, interpolation = cv2.INTER_CUBIC )\n",
        "  images_left.append(cv2.cvtColor(left_img, cv2.COLOR_BGR2GRAY).astype('float32')/255.)\n",
        "  images_left_bgr.append(left_img)\n",
        "\n",
        "\n",
        "for file in tqdm(right_files_path):\n",
        "  right_image_sat= cv2.imread(file)\n",
        "  lab = cv2.cvtColor(right_image_sat, cv2.COLOR_BGR2LAB)\n",
        "  lab[...,0] = clahe.apply(lab[...,0])\n",
        "  right_image_sat = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "  right_img = cv2.resize(right_image_sat,None,fx=0.50,fy=0.50, interpolation = cv2.INTER_CUBIC )\n",
        "  images_right.append(cv2.cvtColor(right_img, cv2.COLOR_BGR2GRAY).astype('float32')/255.)\n",
        "  images_right_bgr.append(right_img)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [01:08<00:00,  1.34s/it]\n",
            "100%|██████████| 50/50 [01:03<00:00,  1.27s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4qm7wwBUdR7",
        "outputId": "89b72152-a4be-49b6-f992-1852f3d5ccea"
      },
      "source": [
        "f=h5.File(f'drive/MyDrive/all_images_bgr_{Dataset}.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=images_left_bgr + images_right_bgr)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize(f'drive/MyDrive/all_images_bgr_{Dataset}.h5')/1.e6,'MB')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HDF5  w/o comp.: 14.399759769439697 [s] ... size 1818.002048 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoJrDXV0UhpY",
        "outputId": "015950ad-9606-432a-d8fa-a4fd5d3f6fb4"
      },
      "source": [
        "f=h5.File(f'drive/MyDrive/all_images_gray_{Dataset}.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=images_left + images_right)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize(f'drive/MyDrive/all_images_gray_{Dataset}.h5')/1.e6,'MB')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HDF5  w/o comp.: 37.58129620552063 [s] ... size 2424.002048 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgMi_K_ogLZv",
        "outputId": "66b916b1-de70-4aba-df52-152f04dee2d3"
      },
      "source": [
        "f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=images_left_bgr + images_right_bgr)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/all_images_bgr_sift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HDF5  w/o comp.: 30.252750873565674 [s] ... size 1818.002048 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8nBRUUmgSss",
        "outputId": "2786e4d5-7e47-489d-87ad-71f2e3baf961"
      },
      "source": [
        "f=h5.File('drive/MyDrive/all_images_gray_sift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=images_left + images_right)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/all_images_gray_sift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HDF5  w/o comp.: 40.11842608451843 [s] ... size 2424.002048 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JClT70JjrP7"
      },
      "source": [
        "del images_left_bgr,images_right_bgr"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8wlGYSTjt1s"
      },
      "source": [
        "\n",
        "from timeit import default_timer as timer\n",
        "time_all = []"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1odyDIEDjx_F"
      },
      "source": [
        "num_kps_sift = []\n",
        "num_kps_brisk = []\n",
        "num_kps_agast = []\n",
        "num_kps_kaze = []\n",
        "num_kps_akaze = []\n",
        "num_kps_orb = []\n",
        "num_kps_mser = []\n",
        "num_kps_daisy = []\n",
        "num_kps_surfsift = []\n",
        "num_kps_freak = []\n",
        "num_kps_gftt = []\n",
        "num_kps_briefstar = []\n",
        "num_kps_surf = []\n",
        "num_kps_rootsift = []"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XkGHfp7hkMn",
        "outputId": "e9bcb998-1b76-4087-c8cc-cc4deaac5c6b"
      },
      "source": [
        "Threshl=60;\n",
        "Octaves=6; \n",
        "#PatternScales=1.0f;\n",
        "\n",
        "start = timer()\n",
        "\n",
        "brisk = cv2.BRISK_create(Threshl,Octaves)\n",
        "\n",
        "\n",
        "keypoints_all_left_brisk = []\n",
        "descriptors_all_left_brisk = []\n",
        "points_all_left_brisk=[]\n",
        "\n",
        "keypoints_all_right_brisk = []\n",
        "descriptors_all_right_brisk = []\n",
        "points_all_right_brisk=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()  \n",
        "  kpt = brisk.detect(imgs,None)\n",
        "  kpt,descrip =  brisk.compute(imgs, kpt)\n",
        "  keypoints_all_left_brisk.append(kpt)\n",
        "  descriptors_all_left_brisk.append(descrip)\n",
        "  #points_all_left_brisk.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = brisk.detect(imgs,None)\n",
        "  kpt,descrip =  brisk.compute(imgs, kpt)\n",
        "  keypoints_all_right_brisk.append(kpt)\n",
        "  descriptors_all_right_brisk.append(descrip)\n",
        "  #points_all_right_brisk.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [03:43<00:00,  4.38s/it]\n",
            "100%|██████████| 50/50 [03:03<00:00,  3.66s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNkt2Q95hneQ",
        "outputId": "3b9467e4-3cf7-4bce-abee-8fd94522d7e1"
      },
      "source": [
        "for j in tqdm(keypoints_all_left_brisk + keypoints_all_right_brisk[1:]):\n",
        "  num_kps_brisk.append(len(j))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 351576.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcwVmUUBhqGl"
      },
      "source": [
        "all_feat_brisk_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_brisk):\n",
        "  all_feat_brisk_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_brisk[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_brisk_left_each.append(temp)\n",
        "  all_feat_brisk_left.append(all_feat_brisk_left_each)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0urwaGfohsdT"
      },
      "source": [
        "all_feat_brisk_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_brisk):\n",
        "  all_feat_brisk_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_brisk[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_brisk_right_each.append(temp)\n",
        "  all_feat_brisk_right.append(all_feat_brisk_right_each)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCvWwY-JhuhS"
      },
      "source": [
        "del keypoints_all_left_brisk, keypoints_all_right_brisk, descriptors_all_left_brisk, descriptors_all_right_brisk"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk2bM7Iyhy1H"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_brisk_left.dat', 'wb')\n",
        "pickle.dump(all_feat_brisk_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIOZq4jkh09o"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_brisk_right.dat', 'wb')\n",
        "pickle.dump(all_feat_brisk_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Zr-z75h5CZ"
      },
      "source": [
        "del Fdb, all_feat_brisk_left, all_feat_brisk_right"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6kT86LxlYSs"
      },
      "source": [
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "tqdm = partial(tqdm, position=0, leave=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAASWfb0lcwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7de989d-ebe0-45d1-9910-c9876ddf48f1"
      },
      "source": [
        "start = timer()\n",
        "\n",
        "akaze = cv2.AKAZE_create()\n",
        "\n",
        "\n",
        "keypoints_all_left_akaze = []\n",
        "descriptors_all_left_akaze = []\n",
        "points_all_left_akaze=[]\n",
        "\n",
        "keypoints_all_right_akaze = []\n",
        "descriptors_all_right_akaze = []\n",
        "points_all_right_akaze=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = akaze.detect(imgs,None)\n",
        "  kpt,descrip =  akaze.compute(imgs, kpt)\n",
        "  keypoints_all_left_akaze.append(kpt)\n",
        "  descriptors_all_left_akaze.append(descrip)\n",
        "  #points_all_left_akaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = akaze.detect(imgs,None)\n",
        "  kpt,descrip = akaze.compute(imgs, kpt)\n",
        "  keypoints_all_right_akaze.append(kpt)\n",
        "  descriptors_all_right_akaze.append(descrip)\n",
        "  #points_all_right_akaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [04:19<00:00,  5.08s/it]\n",
            "100%|██████████| 50/50 [03:57<00:00,  4.76s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw5dtNPTlz05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1fe128-409c-41e7-b205-789e88e9b76d"
      },
      "source": [
        "for j in tqdm(keypoints_all_left_akaze + keypoints_all_right_akaze[1:]):\n",
        "  num_kps_akaze.append(len(j))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 134303.68it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44am036ul0cA"
      },
      "source": [
        "all_feat_akaze_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_akaze):\n",
        "  all_feat_akaze_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_akaze[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_akaze_left_each.append(temp)\n",
        "  all_feat_akaze_left.append(all_feat_akaze_left_each)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR6fxdgGw69d"
      },
      "source": [
        "del keypoints_all_left_akaze, descriptors_all_left_akaze"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm3Ipay0l2ZV"
      },
      "source": [
        "all_feat_akaze_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_akaze):\n",
        "  all_feat_akaze_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_akaze[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_akaze_right_each.append(temp)\n",
        "  all_feat_akaze_right.append(all_feat_akaze_right_each)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Sm6SLll8zs"
      },
      "source": [
        "del  keypoints_all_right_akaze,  descriptors_all_right_akaze"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sF5xhHBl_Hf"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_akaze_left.dat', 'wb')\n",
        "pickle.dump(all_feat_akaze_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxOACudlxEh_"
      },
      "source": [
        "del Fdb, all_feat_akaze_left"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpVRlV9hmCEZ"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_akaze_right.dat', 'wb')\n",
        "pickle.dump(all_feat_akaze_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsjjNwatmES1"
      },
      "source": [
        "del Fdb, all_feat_akaze_right"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j5hTI_2mGdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f83c45e-b29b-4788-b7ce-3f8406a3edb8"
      },
      "source": [
        "orb = cv2.ORB_create(20000)\n",
        "\n",
        "start = timer()\n",
        "\n",
        "\n",
        "keypoints_all_left_orb = []\n",
        "descriptors_all_left_orb = []\n",
        "points_all_left_orb=[]\n",
        "\n",
        "keypoints_all_right_orb = []\n",
        "descriptors_all_right_orb = []\n",
        "points_all_right_orb=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()    \n",
        "  kpt = orb.detect(imgs,None)\n",
        "  kpt,descrip =  orb.compute(imgs, kpt)\n",
        "  keypoints_all_left_orb.append(kpt)\n",
        "  descriptors_all_left_orb.append(descrip)\n",
        "  #points_all_left_orb.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = orb.detect(imgs,None)\n",
        "  kpt,descrip =  orb.compute(imgs, kpt)\n",
        "  keypoints_all_right_orb.append(kpt)\n",
        "  descriptors_all_right_orb.append(descrip)\n",
        "  #points_all_right_orb.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [00:46<00:00,  1.09it/s]\n",
            "100%|██████████| 50/50 [00:30<00:00,  1.64it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VElsJi7mmLau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e020cd-e594-488b-fce2-c8f4a7112bcc"
      },
      "source": [
        "for j in tqdm(keypoints_all_left_orb + keypoints_all_right_orb[1:]):\n",
        "  num_kps_orb.append(len(j))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 102450.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukP2Vg93mOTz"
      },
      "source": [
        "all_feat_orb_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_orb):\n",
        "  all_feat_orb_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_orb[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_orb_left_each.append(temp)\n",
        "  all_feat_orb_left.append(all_feat_orb_left_each)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTc5HtCGmQVS"
      },
      "source": [
        "all_feat_orb_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_orb):\n",
        "  all_feat_orb_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_orb[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_orb_right_each.append(temp)\n",
        "  all_feat_orb_right.append(all_feat_orb_right_each)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr36tn-imSRN"
      },
      "source": [
        "del keypoints_all_left_orb, keypoints_all_right_orb, descriptors_all_left_orb, descriptors_all_right_orb"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgupbJ5KmVIH"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_orb_left.dat', 'wb')\n",
        "pickle.dump(all_feat_orb_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LSw_x9zmW0F"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_orb_right.dat', 'wb')\n",
        "pickle.dump(all_feat_orb_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nASB0xk5mYdw"
      },
      "source": [
        "del Fdb, all_feat_orb_left, all_feat_orb_right"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mEDQ6_fmbKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81414b77-cdc6-4c50-8bb3-24a4fcd9e46a"
      },
      "source": [
        "start = timer()\n",
        "\n",
        "star = cv2.xfeatures2d.StarDetector_create()\n",
        "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
        "\n",
        "keypoints_all_left_star = []\n",
        "descriptors_all_left_brief = []\n",
        "points_all_left_star=[]\n",
        "\n",
        "keypoints_all_right_star = []\n",
        "descriptors_all_right_brief = []\n",
        "points_all_right_star=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = star.detect(imgs,None)\n",
        "  kpt,descrip =  brief.compute(imgs, kpt)\n",
        "  keypoints_all_left_star.append(kpt)\n",
        "  descriptors_all_left_brief.append(descrip)\n",
        "  #points_all_left_star.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = star.detect(imgs,None)\n",
        "  kpt,descrip =  brief.compute(imgs, kpt)\n",
        "  keypoints_all_right_star.append(kpt)\n",
        "  descriptors_all_right_brief.append(descrip)\n",
        "  #points_all_right_star.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [00:39<00:00,  1.30it/s]\n",
            "100%|██████████| 50/50 [00:37<00:00,  1.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKUaXJ6FmqYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039c6e4d-e5b6-4c2e-bc04-e3a2e515784c"
      },
      "source": [
        "num_kps_star=[]\n",
        "for j in tqdm(keypoints_all_left_star + keypoints_all_right_star[1:]):\n",
        "  num_kps_star.append(len(j))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 391625.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbcJfKqWmsqY"
      },
      "source": [
        "all_feat_star_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_star):\n",
        "  all_feat_star_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_brief[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_star_left_each.append(temp)\n",
        "  all_feat_star_left.append(all_feat_star_left_each)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Alx1W-EmtKW"
      },
      "source": [
        "all_feat_star_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_star):\n",
        "  all_feat_star_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_brief[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_star_right_each.append(temp)\n",
        "  all_feat_star_right.append(all_feat_star_right_each)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNgXcpN0mvYG"
      },
      "source": [
        "del keypoints_all_left_star, keypoints_all_right_star, descriptors_all_left_brief, descriptors_all_right_brief"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDXglZ_Nmy2L"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_star_left.dat', 'wb')\n",
        "pickle.dump(all_feat_star_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1K38OWSm0pZ"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_star_right.dat', 'wb')\n",
        "pickle.dump(all_feat_star_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiH7G6Jdm2hG"
      },
      "source": [
        "del Fdb, all_feat_star_left, all_feat_star_right"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwOfkXf4m8ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14604dcd-3e47-4b1c-e5f1-8b0faacd8cee"
      },
      "source": [
        "start = timer()\n",
        "\n",
        "Threshl=60;\n",
        "Octaves=8; \n",
        "#PatternScales=1.0f;\n",
        "brisk = cv2.BRISK_create(Threshl,Octaves)\n",
        "\n",
        "freak = cv2.xfeatures2d.FREAK_create()\n",
        "keypoints_all_left_freak = []\n",
        "descriptors_all_left_freak = []\n",
        "points_all_left_freak=[]\n",
        "\n",
        "keypoints_all_right_freak = []\n",
        "descriptors_all_right_freak = []\n",
        "points_all_right_freak=[]\n",
        "\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = brisk.detect(imgs)\n",
        "  kpt,descrip =  freak.compute(imgs, kpt)\n",
        "  keypoints_all_left_freak.append(kpt)\n",
        "  descriptors_all_left_freak.append(descrip)\n",
        "  #points_all_left_freak.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = brisk.detect(imgs,None)\n",
        "  kpt,descrip =  freak.compute(imgs, kpt)\n",
        "  keypoints_all_right_freak.append(kpt)\n",
        "  descriptors_all_right_freak.append(descrip)\n",
        "  #points_all_right_freak.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [03:26<00:00,  4.04s/it]\n",
            "100%|██████████| 50/50 [02:57<00:00,  3.56s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvMe9-WVnA8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c7d0dd-4ebb-43c9-f576-b8c995483864"
      },
      "source": [
        "num_kps_freak=[]\n",
        "for j in tqdm(keypoints_all_left_freak + keypoints_all_right_freak[1:]):\n",
        "  num_kps_freak.append(len(j))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 428427.37it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I24GkbY1nBm5"
      },
      "source": [
        "all_feat_freak_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_freak):\n",
        "  all_feat_freak_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_freak[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_freak_left_each.append(temp)\n",
        "  all_feat_freak_left.append(all_feat_freak_left_each)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF0FaUjCnDa5"
      },
      "source": [
        "all_feat_freak_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_freak):\n",
        "  all_feat_freak_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_freak[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_freak_right_each.append(temp)\n",
        "  all_feat_freak_right.append(all_feat_freak_right_each)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPQYNqAD1Jt0"
      },
      "source": [
        "del keypoints_all_left_freak, keypoints_all_right_freak, descriptors_all_left_freak, descriptors_all_right_freak"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYhGLgT4nHq4"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_freak_left.dat', 'wb')\n",
        "pickle.dump(all_feat_freak_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKQgZGL13jas"
      },
      "source": [
        "del Fdb, all_feat_freak_left"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycUWAHjDnJdH"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_freak_right.dat', 'wb')\n",
        "pickle.dump(all_feat_freak_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpG4UVJunNvz"
      },
      "source": [
        "del Fdb, all_feat_freak_right"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvJYkrnWwHuB"
      },
      "source": [
        "class RootSIFT:\n",
        "  def __init__(self):\n",
        "    # initialize the SIFT feature extractor\n",
        "    #self.extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
        "    self.sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "  def compute(self, image, kps, eps=1e-7):\n",
        "    # compute SIFT descriptors\n",
        "    (kps, descs) = self.sift.compute(image, kps)\n",
        "\n",
        "    # if there are no keypoints or descriptors, return an empty tuple\n",
        "    if len(kps) == 0:\n",
        "      return ([], None)\n",
        "\n",
        "    # apply the Hellinger kernel by first L1-normalizing, taking the\n",
        "    # square-root, and then L2-normalizing\n",
        "    descs /= (np.linalg.norm(descs, axis=0, ord=2) + eps)\n",
        "    descs /= (descs.sum(axis=0) + eps)\n",
        "    descs = np.sqrt(descs)\n",
        "    #descs /= (np.linalg.norm(descs, axis=0, ord=2) + eps)\n",
        "\n",
        "    # return a tuple of the keypoints and descriptors\n",
        "    return (kps, descs)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2mCLe_jwIXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfef8d50-80db-4ff7-b3f5-4868fe7e56c8"
      },
      "source": [
        "start = timer()\n",
        "threshl=60;\n",
        "sift = cv2.xfeatures2d.SIFT_create(threshl)\n",
        "rootsift = RootSIFT()\n",
        "keypoints_all_left_rootsift = []\n",
        "descriptors_all_left_rootsift = []\n",
        "points_all_left_rootsift=[]\n",
        "\n",
        "keypoints_all_right_rootsift = []\n",
        "descriptors_all_right_rootsift = []\n",
        "points_all_right_rootsift=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()  \n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  rootsift.compute(imgs, kpt)\n",
        "  keypoints_all_left_rootsift.append(kpt)\n",
        "  descriptors_all_left_rootsift.append(descrip)\n",
        "  #points_all_left_rootsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  rootsift.compute(imgs, kpt)\n",
        "  keypoints_all_right_rootsift.append(kpt)\n",
        "  descriptors_all_right_rootsift.append(descrip)\n",
        "  #points_all_right_rootsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [03:36<00:00,  4.25s/it]\n",
            "100%|██████████| 50/50 [03:24<00:00,  4.10s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftaahpVKwMpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a34986c-20b0-44f5-ed8c-81685f7f8fb2"
      },
      "source": [
        "num_kps_rootsift=[]\n",
        "for j in tqdm(keypoints_all_left_rootsift + keypoints_all_right_rootsift[1:]):\n",
        "  num_kps_rootsift.append(len(j))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 406819.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3W_2dzOwNEq"
      },
      "source": [
        "all_feat_rootsift_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_rootsift):\n",
        "  all_feat_rootsift_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_rootsift[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_rootsift_left_each.append(temp)\n",
        "  all_feat_rootsift_left.append(all_feat_rootsift_left_each)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_T29D6ZwOoO"
      },
      "source": [
        "all_feat_rootsift_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_rootsift):\n",
        "  all_feat_rootsift_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_rootsift[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_rootsift_right_each.append(temp)\n",
        "  all_feat_rootsift_right.append(all_feat_rootsift_right_each)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pGaEWmnwRFo"
      },
      "source": [
        "del keypoints_all_left_rootsift, keypoints_all_right_rootsift, descriptors_all_left_rootsift, descriptors_all_right_rootsift"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Bi56w4wS48"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_rootsift_left.dat', 'wb')\n",
        "pickle.dump(all_feat_rootsift_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr9DBTn2wVeL"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_rootsift_right.dat', 'wb')\n",
        "pickle.dump(all_feat_rootsift_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm_SD65xwXVF"
      },
      "source": [
        "del Fdb, all_feat_rootsift_left, all_feat_rootsift_right"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz-m4Kvin9Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6d484a-3d46-4201-f9ef-7043128b1be9"
      },
      "source": [
        "start = timer()\n",
        "\n",
        "gftt = cv2.GFTTDetector_create()\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "keypoints_all_left_gftt = []\n",
        "descriptors_all_left_gftt = []\n",
        "points_all_left_gftt=[]\n",
        "\n",
        "keypoints_all_right_gftt = []\n",
        "descriptors_all_right_gftt = []\n",
        "points_all_right_gftt=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = gftt.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_left_gftt.append(kpt)\n",
        "  descriptors_all_left_gftt.append(descrip)\n",
        "  #points_all_left_gftt.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = gftt.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_right_gftt.append(kpt)\n",
        "  descriptors_all_right_gftt.append(descrip)\n",
        "  #points_all_right_gftt.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [00:42<00:00,  1.21it/s]\n",
            "100%|██████████| 50/50 [00:41<00:00,  1.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nj5QGtPoGT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceeccb8d-2fb7-41d7-f84d-87b0b8057926"
      },
      "source": [
        "num_kps_gftt=[]\n",
        "for j in tqdm(keypoints_all_left_gftt + keypoints_all_right_gftt[1:]):\n",
        "  num_kps_gftt.append(len(j))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 98875.62it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "topzdBH6oGyw"
      },
      "source": [
        "all_feat_gftt_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_gftt):\n",
        "  all_feat_gftt_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_gftt[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_gftt_left_each.append(temp)\n",
        "  all_feat_gftt_left.append(all_feat_gftt_left_each)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXXP5SchoItv"
      },
      "source": [
        "all_feat_gftt_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_gftt):\n",
        "  all_feat_gftt_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_gftt[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_gftt_right_each.append(temp)\n",
        "  all_feat_gftt_right.append(all_feat_gftt_right_each)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZDGiYK3oMcO"
      },
      "source": [
        "del keypoints_all_left_gftt, keypoints_all_right_gftt, descriptors_all_left_gftt, descriptors_all_right_gftt"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOwt5FH3oMyX"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_gftt_left.dat', 'wb')\n",
        "pickle.dump(all_feat_gftt_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLdTPEProQUt"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_gftt_right.dat', 'wb')\n",
        "pickle.dump(all_feat_gftt_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsqqSZ1GoQtp"
      },
      "source": [
        "del Fdb, all_feat_gftt_left, all_feat_gftt_right"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0f-eVuLoS2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46838c1e-b866-4afe-b86f-7f1fa17ad5e4"
      },
      "source": [
        "start = timer()\n",
        "threshl=70;\n",
        "daisy = cv2.xfeatures2d.DAISY_create(threshl)\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "keypoints_all_left_daisy = []\n",
        "descriptors_all_left_daisy = []\n",
        "points_all_left_daisy=[]\n",
        "\n",
        "keypoints_all_right_daisy = []\n",
        "descriptors_all_right_daisy = []\n",
        "points_all_right_daisy=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  daisy.compute(imgs, kpt)\n",
        "  keypoints_all_left_daisy.append(kpt)\n",
        "  descriptors_all_left_daisy.append(descrip)\n",
        "  #points_all_left_daisy.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  daisy.compute(imgs, kpt)\n",
        "  keypoints_all_right_daisy.append(kpt)\n",
        "  descriptors_all_right_daisy.append(descrip)\n",
        "  #points_all_right_daisy.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [07:48<00:00,  9.18s/it]\n",
            " 38%|███▊      | 19/50 [02:50<04:45,  9.23s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX0qr1_aodqW"
      },
      "source": [
        "num_kps_daisy=[]\n",
        "for j in tqdm(keypoints_all_left_daisy + keypoints_all_right_daisy[1:]):\n",
        "  num_kps_daisy.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMkEBezXoeig"
      },
      "source": [
        "all_feat_daisy_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_daisy):\n",
        "  all_feat_daisy_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_daisy[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_daisy_left_each.append(temp)\n",
        "  all_feat_daisy_left.append(all_feat_daisy_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO7gePAqoh4Z"
      },
      "source": [
        "all_feat_daisy_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_daisy):\n",
        "  all_feat_daisy_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_daisy[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_daisy_right_each.append(temp)\n",
        "  all_feat_daisy_right.append(all_feat_daisy_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vN5qQw5okXh"
      },
      "source": [
        "del keypoints_all_left_daisy, keypoints_all_right_daisy, descriptors_all_left_daisy, descriptors_all_right_daisy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O2-fvMXok52"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_daisy_left.dat', 'wb')\n",
        "pickle.dump(all_feat_daisy_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-cirIXPonMN"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_daisy_right.dat', 'wb')\n",
        "pickle.dump(all_feat_daisy_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Ss0HnGopUA"
      },
      "source": [
        "del Fdb, all_feat_daisy_left, all_feat_daisy_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IgO7HQzpF-m"
      },
      "source": [
        "start = timer()\n",
        "threshl=70;\n",
        "sift = cv2.xfeatures2d.SIFT_create(threshl)\n",
        "keypoints_all_left_sift = []\n",
        "descriptors_all_left_sift = []\n",
        "points_all_left_sift=[]\n",
        "\n",
        "keypoints_all_right_sift = []\n",
        "descriptors_all_right_sift = []\n",
        "points_all_right_sift=[]\n",
        "\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()\n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_left_sift.append(kpt)\n",
        "  descriptors_all_left_sift.append(descrip)\n",
        "  #points_all_left_sift.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()\n",
        "  kpt = sift.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_right_sift.append(kpt)\n",
        "  descriptors_all_right_sift.append(descrip)\n",
        "  #points_all_right_sift.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQu_N-pqpMit"
      },
      "source": [
        "num_kps_sift=[]\n",
        "for j in tqdm(keypoints_all_left_sift + keypoints_all_right_sift[1:]):\n",
        "  num_kps_sift.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH8va4nFpNEc"
      },
      "source": [
        "all_feat_sift_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_sift):\n",
        "  all_feat_sift_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_sift[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_sift_left_each.append(temp)\n",
        "  all_feat_sift_left.append(all_feat_sift_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am4BJlrlpO9a"
      },
      "source": [
        "all_feat_sift_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_sift):\n",
        "  all_feat_sift_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_sift[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_sift_right_each.append(temp)\n",
        "  all_feat_sift_right.append(all_feat_sift_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5t-ITF1pQwO"
      },
      "source": [
        "del  keypoints_all_right_sift, descriptors_all_right_sift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSbLAzZBpSqr"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_sift_left.dat', 'wb')\n",
        "pickle.dump(all_feat_sift_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1zVg0c9Ut2Q"
      },
      "source": [
        "del Fdb, all_feat_sift_left"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYDmgF6spVtI"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_sift_right.dat', 'wb')\n",
        "pickle.dump(all_feat_sift_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SJWIQeMpXiM"
      },
      "source": [
        "del Fdb, all_feat_sift_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQpbH8U6pZWy"
      },
      "source": [
        "start = timer()\n",
        "threshl=70;\n",
        "surf  = cv2.xfeatures2d.SURF_create(threshl)\n",
        "keypoints_all_left_surf = []\n",
        "descriptors_all_left_surf = []\n",
        "points_all_left_surf=[]\n",
        "\n",
        "keypoints_all_right_surf = []\n",
        "descriptors_all_right_surf = []\n",
        "points_all_right_surf=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()  \n",
        "  kpt = surf.detect(imgs,None)\n",
        "  kpt,descrip =  surf.compute(imgs, kpt)\n",
        "  keypoints_all_left_surf.append(kpt)\n",
        "  descriptors_all_left_surf.append(descrip)\n",
        "  #points_all_left_surf.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = surf.detect(imgs,None)\n",
        "  kpt,descrip =  surf.compute(imgs, kpt)\n",
        "  keypoints_all_right_surf.append(kpt)\n",
        "  descriptors_all_right_surf.append(descrip)\n",
        "  #points_all_right_surf.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "377Oz6G7piyJ"
      },
      "source": [
        "num_kps_surf=[]\n",
        "for j in tqdm(keypoints_all_left_surf + keypoints_all_right_surf[1:]):\n",
        "  num_kps_surf.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhcNfsnCplpt"
      },
      "source": [
        "all_feat_surf_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_surf):\n",
        "  all_feat_surf_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_surf[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_surf_left_each.append(temp)\n",
        "  all_feat_surf_left.append(all_feat_surf_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqZvVjTBUlaI"
      },
      "source": [
        "del keypoints_all_left_surf, descriptors_all_left_surf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIZ9I2Gdpnfd"
      },
      "source": [
        "all_feat_surf_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_surf):\n",
        "  all_feat_surf_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_surf[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_surf_right_each.append(temp)\n",
        "  all_feat_surf_right.append(all_feat_surf_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvOHyqYopptx"
      },
      "source": [
        "del  keypoints_all_right_surf, descriptors_all_right_surf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFEtp5F4ppw6"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_surf_left.dat', 'wb')\n",
        "pickle.dump(all_feat_surf_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10lvDWwCUfc8"
      },
      "source": [
        "del Fdb, all_feat_surf_left"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TQWD1aNpumG"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_surf_right.dat', 'wb')\n",
        "pickle.dump(all_feat_surf_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utbY7exepxmq"
      },
      "source": [
        "del Fdb, all_feat_surf_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CdbjUk-kbea"
      },
      "source": [
        "start = timer()\n",
        "threshl=70;\n",
        "mser = cv2.MSER_create()\n",
        "sift = cv2.xfeatures2d.SIFT_create(threshl)\n",
        "\n",
        "keypoints_all_left_mser = []\n",
        "descriptors_all_left_mser = []\n",
        "points_all_left_mser=[]\n",
        "\n",
        "keypoints_all_right_mser = []\n",
        "descriptors_all_right_mser = []\n",
        "points_all_right_mser=[]\n",
        "\n",
        "for cnt in tqdm(range(len(left_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt]\n",
        "  f.close()     \n",
        "  kpt = mser.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_left_mser.append(kpt)\n",
        "  descriptors_all_left_mser.append(descrip)\n",
        "  #points_all_left_mser.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "for cnt in tqdm(range(len(right_files_path))):\n",
        "  f=h5.File('drive/MyDrive/all_images_bgr_sift_40.h5','r')\n",
        "  imgs = f['data'][cnt+len(left_files_path)]\n",
        "  f.close()  \n",
        "  kpt = mser.detect(imgs,None)\n",
        "  kpt,descrip =  sift.compute(imgs, kpt)\n",
        "  keypoints_all_right_mser.append(kpt)\n",
        "  descriptors_all_right_mser.append(descrip)\n",
        "  #points_all_right_mser.append(np.asarray([[p.pt[0], p.pt[1]] for p in kpt]))\n",
        "\n",
        "end = timer()\n",
        "\n",
        "time_all.append(end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOpw8j5joT2w"
      },
      "source": [
        "num_kps_mser=[]\n",
        "for j in tqdm(keypoints_all_left_mser + keypoints_all_right_mser[1:]):\n",
        "  num_kps_mser.append(len(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzJuChTtoV4J"
      },
      "source": [
        "all_feat_mser_left = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_left_mser):\n",
        "  all_feat_mser_left_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_left_mser[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_mser_left_each.append(temp)\n",
        "  all_feat_mser_left.append(all_feat_mser_left_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jjB2N-SEpoj"
      },
      "source": [
        "del keypoints_all_left_mser, descriptors_all_left_mser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHfHELOqoWVK"
      },
      "source": [
        "all_feat_mser_right = []\n",
        "for cnt,kpt_all in enumerate(keypoints_all_right_mser):\n",
        "  all_feat_mser_right_each = []\n",
        "  for cnt_each, kpt in enumerate(kpt_all):\n",
        "    desc = descriptors_all_right_mser[cnt][cnt_each]\n",
        "    temp = (kpt.pt, kpt.size, kpt.angle, kpt.response, kpt.octave, \n",
        "        kpt.class_id, desc)\n",
        "    all_feat_mser_right_each.append(temp)\n",
        "  all_feat_mser_right.append(all_feat_mser_right_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5wjyybWoYjP"
      },
      "source": [
        "del  keypoints_all_right_mser,  descriptors_all_right_mser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu_mzTrkocPT"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_mser_left.dat', 'wb')\n",
        "pickle.dump(all_feat_mser_left,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VpTUMFoExwp"
      },
      "source": [
        "del Fdb, all_feat_mser_left"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VcH2vguofd9"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_mser_right.dat', 'wb')\n",
        "pickle.dump(all_feat_mser_right,Fdb,-1)\n",
        "Fdb.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrc5tydLohdV"
      },
      "source": [
        "del Fdb, all_feat_mser_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTn69MzoqYFq"
      },
      "source": [
        "def compute_homography_fast(matched_pts1, matched_pts2,thresh=4):\n",
        "    #matched_pts1 = cv2.KeyPoint_convert(matched_kp1)\n",
        "    #matched_pts2 = cv2.KeyPoint_convert(matched_kp2)\n",
        "\n",
        "    # Estimate the homography between the matches using RANSAC\n",
        "    H, inliers = cv2.findHomography(matched_pts1,\n",
        "                                    matched_pts2,\n",
        "                                    cv2.RANSAC, ransacReprojThreshold =thresh, maxIters=3000)\n",
        "    inliers = inliers.flatten()\n",
        "    return H, inliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65VuhAuZqaPq"
      },
      "source": [
        "def compute_homography_fast_other(matched_pts1, matched_pts2):\n",
        "    #matched_pts1 = cv2.KeyPoint_convert(matched_kp1)\n",
        "    #matched_pts2 = cv2.KeyPoint_convert(matched_kp2)\n",
        "\n",
        "    # Estimate the homography between the matches using RANSAC\n",
        "    H, inliers = cv2.findHomography(matched_pts1,\n",
        "                                    matched_pts2,\n",
        "                                    0)\n",
        "    inliers = inliers.flatten()\n",
        "    return H, inliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezg7yfBlqfVn"
      },
      "source": [
        "def get_Hmatrix(imgs,keypts,pts,descripts,ratio=0.75,thresh=4,use_lowe=True,disp=False,no_ransac=False,binary=False):\n",
        "  lff1 = descripts[0]\n",
        "  lff = descripts[1]\n",
        "\n",
        "  if use_lowe==False:\n",
        "    #FLANN_INDEX_KDTREE = 2\n",
        "    #index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "    #search_params = dict(checks=50)\n",
        "    #flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "    #flann = cv2.BFMatcher()\n",
        "    if binary==True:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "\n",
        "    else:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "      lff1 = np.float32(descripts[0])\n",
        "      lff = np.float32(descripts[1])\n",
        "\n",
        "\n",
        "    #matches_lf1_lf = flann.knnMatch(lff1, lff, k=2)\n",
        "    matches_4 = bf.knnMatch(lff1, lff,k=2)\n",
        "    matches_lf1_lf = []\n",
        "\n",
        "\n",
        "    print(\"\\nNumber of matches\",len(matches_4))\n",
        "    '''\n",
        "    matches_4 = []\n",
        "    ratio = ratio\n",
        "    # loop over the raw matches\n",
        "    for m in matches_lf1_lf:\n",
        "      # ensure the distance is within a certain ratio of each\n",
        "      # other (i.e. Lowe’s ratio test)\n",
        "      #if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
        "          #matches_1.append((m[0].trainIdx, m[0].queryIdx))\n",
        "      matches_4.append(m[0])\n",
        "    '''\n",
        "    print(\"Number of matches After Lowe's Ratio\",len(matches_4))\n",
        "  else:\n",
        "    FLANN_INDEX_KDTREE = 2\n",
        "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "    search_params = dict(checks=50)\n",
        "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "    if binary==True:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "      lff1 = np.float32(descripts[0])\n",
        "      lff = np.float32(descripts[1])\n",
        "    else:\n",
        "      bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "      lff1 = np.float32(descripts[0])\n",
        "      lff = np.float32(descripts[1])\n",
        "\n",
        "\n",
        "    matches_lf1_lf = flann.knnMatch(lff1, lff, k=2)\n",
        "    #matches_lf1_lf = bf.knnMatch(lff1, lff,k=2)\n",
        "\n",
        "\n",
        "    print(\"\\nNumber of matches\",len(matches_lf1_lf))\n",
        "    matches_4 = []\n",
        "    ratio = ratio\n",
        "    # loop over the raw matches\n",
        "    for m in matches_lf1_lf:\n",
        "      # ensure the distance is within a certain ratio of each\n",
        "      # other (i.e. Lowe’s ratio test)\n",
        "      if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
        "          #matches_1.append((m[0].trainIdx, m[0].queryIdx))\n",
        "        matches_4.append(m[0])\n",
        "  \n",
        "    print(\"Number of matches After Lowe's Ratio\",len(matches_4))\n",
        "\n",
        "\n",
        "  \n",
        "  matches_idx = np.array([m.queryIdx for m in matches_4])\n",
        "  imm1_pts = np.array([keypts[0][idx].pt for idx in matches_idx])\n",
        "  matches_idx = np.array([m.trainIdx for m in matches_4])\n",
        "  imm2_pts = np.array([keypts[1][idx].pt for idx in matches_idx])\n",
        "  if no_ransac==True:\n",
        "    Hn,inliers = compute_homography_fast_other(imm1_pts,imm2_pts)\n",
        "  else:\n",
        "    Hn,inliers = compute_homography_fast(imm1_pts,imm2_pts,thresh)  \n",
        "\n",
        "  inlier_matchset = np.array(matches_4)[inliers.astype(bool)].tolist()\n",
        "  print(\"Number of Robust matches\",len(inlier_matchset))\n",
        "  print(\"\\n\")\n",
        "  \n",
        "  if len(inlier_matchset)<25:\n",
        "    matches_4 = []\n",
        "    ratio = 0.85\n",
        "    # loop over the raw matches\n",
        "    for m in matches_lf1_lf:\n",
        "      # ensure the distance is within a certain ratio of each\n",
        "      # other (i.e. Lowe’s ratio test)\n",
        "      if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
        "          #matches_1.append((m[0].trainIdx, m[0].queryIdx))\n",
        "          matches_4.append(m[0])\n",
        "    print(\"Number of matches After Lowe's Ratio New\",len(matches_4))\n",
        "  \n",
        "    matches_idx = np.array([m.queryIdx for m in matches_4])\n",
        "    imm1_pts = np.array([keypts[0][idx].pt for idx in matches_idx])\n",
        "    matches_idx = np.array([m.trainIdx for m in matches_4])\n",
        "    imm2_pts = np.array([keypts[1][idx].pt for idx in matches_idx])\n",
        "    Hn,inliers = compute_homography_fast(imm1_pts,imm2_pts)  \n",
        "    inlier_matchset = np.array(matches_4)[inliers.astype(bool)].tolist()\n",
        "    print(\"Number of Robust matches New\",len(inlier_matchset))\n",
        "    print(\"\\n\")    \n",
        "  \n",
        "  #H=compute_Homography(imm1_pts,imm2_pts) \n",
        "  #Robustly estimate Homography 1 using RANSAC\n",
        "  #Hn=RANSAC_alg(keypts[0] ,keypts[1], matches_4,  nRANSAC=1500, RANSACthresh=6)\n",
        "\n",
        "  #global inlier_matchset   \n",
        "  \n",
        "  if disp==True:\n",
        "    dispimg1=cv2.drawMatches(imgs[0], keypts[0], imgs[1], keypts[1], inlier_matchset, None,flags=2)\n",
        "    displayplot(dispimg1,'Robust Matching between Reference Image and Right Image ')\n",
        "\n",
        "\n",
        "  return Hn/Hn[2,2], len(matches_lf1_lf), len(inlier_matchset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A39RCm0iquJL"
      },
      "source": [
        "def get_Hmatrix_rfnet(imgs,pts,descripts,disp=True):\n",
        "\n",
        "  des1 = descripts[0]\n",
        "  des2 = descripts[1]\n",
        "\n",
        "  kp1 = pts[0]\n",
        "  kp2 = pts[1]\n",
        "\n",
        "\n",
        "  predict_label, nn_kp2 = nearest_neighbor_distance_ratio_match(des1, des2, kp2, 0.7)\n",
        "  idx = predict_label.nonzero().view(-1)\n",
        "  mkp1 = kp1.index_select(dim=0, index=idx.long())  # predict match keypoints in I1\n",
        "  mkp2 = nn_kp2.index_select(dim=0, index=idx.long())  # predict match keypoints in I2\n",
        "\n",
        "  #img1, img2 = reverse_img(img1), reverse_img(img2)\n",
        "  keypoints1 = list(map(to_cv2_kp, mkp1))\n",
        "  keypoints2 = list(map(to_cv2_kp, mkp2))\n",
        "  DMatch = list(map(to_cv2_dmatch, np.arange(0, len(keypoints1))))\n",
        "\n",
        "  imm1_pts=np.empty((len(DMatch),2))\n",
        "  imm2_pts=np.empty((len(DMatch),2))\n",
        "  for i in range(0,len(DMatch)):\n",
        "    m = DMatch[i]\n",
        "    (a_x, a_y) = keypoints1[m.queryIdx].pt\n",
        "    (b_x, b_y) = keypoints2[m.trainIdx].pt\n",
        "    imm1_pts[i]=(a_x, a_y)\n",
        "    imm2_pts[i]=(b_x, b_y)    \n",
        "  H=compute_Homography_fast(imm1_pts,imm2_pts) \n",
        "\n",
        "\n",
        "  if disp==True:\n",
        "    dispimg1 = cv2.drawMatches(imgs[0], keypoints1, imgs[1], keypoints2, DMatch, None)\n",
        "    displayplot(dispimg1,'Robust Matching between Reference Image and Right Image ')\n",
        "\n",
        "\n",
        "  return H/H[2,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPfo5m6WqFhB"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_brisk_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_brisk = []\n",
        "descriptors_all_left_brisk = []\n",
        "points_all_left_brisk = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_brisk.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_brisk.append(keypoints_each)\n",
        "  descriptors_all_left_brisk.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqli6vcdqGOg"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_brisk_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_brisk = []\n",
        "descriptors_all_right_brisk = []\n",
        "points_all_right_brisk = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_brisk.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_brisk.append(keypoints_each)\n",
        "  descriptors_all_right_brisk.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_ANX_rnqIjl"
      },
      "source": [
        "H_left_brisk = []\n",
        "H_right_brisk = []\n",
        "\n",
        "num_matches_brisk = []\n",
        "num_good_matches_brisk = []\n",
        "\n",
        "images_left_bgr = []\n",
        "images_right_bgr = []\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_brisk[j:j+2][::-1],points_all_left_brisk[j:j+2][::-1],descriptors_all_left_brisk[j:j+2][::-1],0.7,3,use_lowe=True,binary=True)\n",
        "  H_left_brisk.append(H_a)\n",
        "  num_matches_brisk.append(matches)\n",
        "  num_good_matches_brisk.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_brisk[j:j+2][::-1],points_all_right_brisk[j:j+2][::-1],descriptors_all_right_brisk[j:j+2][::-1],0.7,3,use_lowe=True,binary=True)\n",
        "  H_right_brisk.append(H_a)\n",
        "  num_matches_brisk.append(matches)\n",
        "  num_good_matches_brisk.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDnXhNR8qLO1"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_brisk_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_brisk)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_brisk_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPBLzmPlqOBT"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_brisk_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_brisk)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_brisk_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2ytjOE4qQVY"
      },
      "source": [
        "del H_left_brisk, H_right_brisk,keypoints_all_left_brisk, keypoints_all_right_brisk, descriptors_all_left_brisk, descriptors_all_right_brisk, points_all_left_brisk, points_all_right_brisk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykNrGWgTqVTV"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_sift_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_sift = []\n",
        "descriptors_all_left_sift = []\n",
        "\n",
        "\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_sift.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_sift.append(keypoints_each)\n",
        "  descriptors_all_left_sift.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZz1rTCZqWHX"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_sift_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_sift = []\n",
        "descriptors_all_right_sift = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_sift.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_sift.append(keypoints_each)\n",
        "  descriptors_all_right_sift.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ6UzM08qYnV"
      },
      "source": [
        "H_left_sift = []\n",
        "H_right_sift = []\n",
        "\n",
        "num_matches_sift = []\n",
        "num_good_matches_sift = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_sift[j:j+2][::-1],points_all_left_sift[j:j+2][::-1],descriptors_all_left_sift[j:j+2][::-1])\n",
        "  H_left_sift.append(H_a)\n",
        "  num_matches_sift.append(matches)\n",
        "  num_good_matches_sift.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_sift[j:j+2][::-1],points_all_right_sift[j:j+2][::-1],descriptors_all_right_sift[j:j+2][::-1])\n",
        "  H_right_sift.append(H_a)\n",
        "  num_matches_sift.append(matches)\n",
        "  num_good_matches_sift.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OnjuYvQqbNI"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_sift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_sift)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_sift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyqiRskWqeRC"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_sift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_sift)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_sift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WQ_mMYsqimL"
      },
      "source": [
        "del H_left_sift, H_right_sift,keypoints_all_left_sift, keypoints_all_right_sift, descriptors_all_left_sift, descriptors_all_right_sift, points_all_left_sift, points_all_right_sift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tspGsabq1WR"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_orb_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_orb = []\n",
        "descriptors_all_left_orb = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_orb.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_orb.append(keypoints_each)\n",
        "  descriptors_all_left_orb.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eSZivteq4Ef"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_orb_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_orb = []\n",
        "descriptors_all_right_orb = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_orb.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_orb.append(keypoints_each)\n",
        "  descriptors_all_right_orb.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXPw5aIkq7Ev"
      },
      "source": [
        "H_left_orb = []\n",
        "H_right_orb = []\n",
        "\n",
        "num_matches_orb = []\n",
        "num_good_matches_orb = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_orb[j:j+2][::-1],points_all_left_orb[j:j+2][::-1],descriptors_all_left_orb[j:j+2][::-1],0.7)\n",
        "  H_left_orb.append(H_a)\n",
        "  num_matches_orb.append(matches)\n",
        "  num_good_matches_orb.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_orb[j:j+2][::-1],points_all_right_orb[j:j+2][::-1],descriptors_all_right_orb[j:j+2][::-1],0.7)\n",
        "  H_right_orb.append(H_a)\n",
        "  num_matches_orb.append(matches)\n",
        "  num_good_matches_orb.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptqptP3jq9qL"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_orb_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_orb)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_orb_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDf1LpNOrAoe"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_orb_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_orb)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_orb_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLdZ6oKLrDf6"
      },
      "source": [
        "del H_left_orb, H_right_orb,keypoints_all_left_orb, keypoints_all_right_orb, descriptors_all_left_orb, descriptors_all_right_orb, points_all_left_orb, points_all_right_orb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maJ569ugrYDS"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_akaze_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_akaze = []\n",
        "descriptors_all_left_akaze = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_akaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_akaze.append(keypoints_each)\n",
        "  descriptors_all_left_akaze.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4cAhu4JrYp4"
      },
      "source": [
        "\n",
        "import pickle\n",
        "Fdb = open('all_feat_akaze_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_akaze = []\n",
        "descriptors_all_right_akaze = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_akaze.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_akaze.append(keypoints_each)\n",
        "  descriptors_all_right_akaze.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxcnZfvTrdx6"
      },
      "source": [
        "H_left_akaze = []\n",
        "H_right_akaze = []\n",
        "\n",
        "num_matches_akaze = []\n",
        "num_good_matches_akaze = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_akaze[j:j+2][::-1],points_all_left_akaze[j:j+2][::-1],descriptors_all_left_akaze[j:j+2][::-1])\n",
        "  H_left_akaze.append(H_a)\n",
        "  num_matches_akaze.append(matches)\n",
        "  num_good_matches_akaze.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_akaze[j:j+2][::-1],points_all_right_akaze[j:j+2][::-1],descriptors_all_right_akaze[j:j+2][::-1])\n",
        "  H_right_akaze.append(H_a)\n",
        "  num_matches_akaze.append(matches)\n",
        "  num_good_matches_akaze.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpxh6tUqreTY"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_akaze_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_akaze)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_akaze_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJZYNAF-riEz"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_akaze_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_akaze)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_akaze_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHObs3PkrmBm"
      },
      "source": [
        "del H_left_akaze, H_right_akaze,keypoints_all_left_akaze, keypoints_all_right_akaze, descriptors_all_left_akaze, descriptors_all_right_akaze, points_all_left_akaze, points_all_right_akaze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EzJPc8_rpIY"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_star_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_star = []\n",
        "descriptors_all_left_brief = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_star.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_star.append(keypoints_each)\n",
        "  descriptors_all_left_brief.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeviiOPsrr5K"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_star_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_star = []\n",
        "descriptors_all_right_brief = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_star.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_star.append(keypoints_each)\n",
        "  descriptors_all_right_brief.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGLcZD9zrsWK"
      },
      "source": [
        "H_left_brief = []\n",
        "H_right_brief = []\n",
        "\n",
        "num_matches_briefstar = []\n",
        "num_good_matches_briefstar = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_star[j:j+2][::-1],points_all_left_star[j:j+2][::-1],descriptors_all_left_brief[j:j+2][::-1])\n",
        "  H_left_brief.append(H_a)\n",
        "  num_matches_briefstar.append(matches)\n",
        "  num_good_matches_briefstar.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_star[j:j+2][::-1],points_all_right_star[j:j+2][::-1],descriptors_all_right_brief[j:j+2][::-1])\n",
        "  H_right_brief.append(H_a)\n",
        "  num_matches_briefstar.append(matches)\n",
        "  num_good_matches_briefstar.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo_hqZ3Xru7T"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_brief_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_brief)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_brief_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qjXsvDXr0da"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_brief_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_brief)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_brief_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tiidG0Xr2p-"
      },
      "source": [
        "del H_left_brief, H_right_brief,keypoints_all_left_star, keypoints_all_right_star, descriptors_all_left_brief, descriptors_all_right_brief, points_all_left_star, points_all_right_star"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5GfAwl3sFqs"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_daisy_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_daisy = []\n",
        "descriptors_all_left_daisy = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_daisy.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_daisy.append(keypoints_each)\n",
        "  descriptors_all_left_daisy.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdYCv_rBsIcO"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_daisy_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_daisy = []\n",
        "descriptors_all_right_daisy = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_daisy.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_daisy.append(keypoints_each)\n",
        "  descriptors_all_right_daisy.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2aUMdg7sLDU"
      },
      "source": [
        "H_left_daisy = []\n",
        "H_right_daisy = []\n",
        "\n",
        "num_matches_daisy = []\n",
        "num_good_matches_daisy = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_daisy[j:j+2][::-1],points_all_left_daisy[j:j+2][::-1],descriptors_all_left_daisy[j:j+2][::-1],1,6)\n",
        "  H_left_daisy.append(H_a)\n",
        "  num_matches_daisy.append(matches)\n",
        "  num_good_matches_daisy.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_daisy[j:j+2][::-1],points_all_right_daisy[j:j+2][::-1],descriptors_all_right_daisy[j:j+2][::-1],1,6)\n",
        "  H_right_daisy.append(H_a)\n",
        "  num_matches_daisy.append(matches)\n",
        "  num_good_matches_daisy.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_iPic1WsNwe"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_daisy_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_daisy)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_daisy_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOhTgILlsQ-s"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_daisy_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_daisy)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_daisy_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhJc3ea0sTfy"
      },
      "source": [
        "del H_left_daisy, H_right_daisy,keypoints_all_left_daisy, keypoints_all_right_daisy, descriptors_all_left_daisy, descriptors_all_right_daisy, points_all_left_daisy, points_all_right_daisy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaeeOrErsVj_"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_freak_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_freak = []\n",
        "descriptors_all_left_freak = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_freak.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_freak.append(keypoints_each)\n",
        "  descriptors_all_left_freak.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qs1Ai7UsagK"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_freak_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_freak = []\n",
        "descriptors_all_right_freak = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_freak.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_freak.append(keypoints_each)\n",
        "  descriptors_all_right_freak.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBJW4ojjsd8y"
      },
      "source": [
        "H_left_freak = []\n",
        "H_right_freak = []\n",
        "\n",
        "num_matches_freak = []\n",
        "num_good_matches_freak = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_freak[j:j+2][::-1],points_all_left_freak[j:j+2][::-1],descriptors_all_left_freak[j:j+2][::-1],0.7,6)\n",
        "  H_left_freak.append(H_a)\n",
        "  num_matches_freak.append(matches)\n",
        "  num_good_matches_freak.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_freak[j:j+2][::-1],points_all_right_freak[j:j+2][::-1],descriptors_all_right_freak[j:j+2][::-1],0.7,6)\n",
        "  H_right_freak.append(H_a)\n",
        "  num_matches_freak.append(matches)\n",
        "  num_good_matches_freak.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsZriYrbskFJ"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_freak_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_freak)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_freak_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2-Vp13askhV"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_freak_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_freak)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_freak_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oyvceq6dsm0k"
      },
      "source": [
        "del H_left_freak, H_right_freak,keypoints_all_left_freak, keypoints_all_right_freak, descriptors_all_left_freak, descriptors_all_right_freak, points_all_left_freak, points_all_right_freak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2DP4SZDsorB"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_surf_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_surf = []\n",
        "descriptors_all_left_surf = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_surf.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_surf.append(keypoints_each)\n",
        "  descriptors_all_left_surf.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgQc_w7ysq9Q"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_surf_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_surf = []\n",
        "descriptors_all_right_surf = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_surf.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_surf.append(keypoints_each)\n",
        "  descriptors_all_right_surf.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBvbn1cRstf0"
      },
      "source": [
        "H_left_surf = []\n",
        "H_right_surf = []\n",
        "\n",
        "num_matches_surf = []\n",
        "num_good_matches_surf = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_surf[j:j+2][::-1],points_all_left_surf[j:j+2][::-1],descriptors_all_left_surf[j:j+2][::-1],0.65)\n",
        "  H_left_surf.append(H_a)\n",
        "  num_matches_surf.append(matches)\n",
        "  num_good_matches_surf.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_surf[j:j+2][::-1],points_all_right_surf[j:j+2][::-1],descriptors_all_right_surf[j:j+2][::-1],0.65)\n",
        "  H_right_surf.append(H_a)\n",
        "  num_matches_surf.append(matches)\n",
        "  num_good_matches_surf.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_E0N3V2sw_b"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_surf_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_surf)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_surf_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEtbEeeIszzS"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_surf_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_surf)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_surf_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYl86ElXs106"
      },
      "source": [
        "del H_left_surf, H_right_surf,keypoints_all_left_surf, keypoints_all_right_surf, descriptors_all_left_surf, descriptors_all_right_surf, points_all_left_surf, points_all_right_surf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CZEHz67s3v0"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_rootsift_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_rootsift = []\n",
        "descriptors_all_left_rootsift = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_rootsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_rootsift.append(keypoints_each)\n",
        "  descriptors_all_left_rootsift.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzg0qf3vs8ZK"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_rootsift_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_rootsift = []\n",
        "descriptors_all_right_rootsift = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_rootsift.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_rootsift.append(keypoints_each)\n",
        "  descriptors_all_right_rootsift.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QYk97zktBEt"
      },
      "source": [
        "H_left_rootsift = []\n",
        "H_right_rootsift = []\n",
        "\n",
        "num_matches_rootsift = []\n",
        "num_good_matches_rootsift = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_rootsift[j:j+2][::-1],points_all_left_rootsift[j:j+2][::-1],descriptors_all_left_rootsift[j:j+2][::-1],0.7)\n",
        "  H_left_rootsift.append(H_a)\n",
        "  num_matches_rootsift.append(matches)\n",
        "  num_good_matches_rootsift.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_rootsift[j:j+2][::-1],points_all_right_rootsift[j:j+2][::-1],descriptors_all_right_rootsift[j:j+2][::-1],0.7)\n",
        "  H_right_rootsift.append(H_a)\n",
        "  num_matches_rootsift.append(matches)\n",
        "  num_good_matches_rootsift.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Vfmt0xtD5f"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_rootsift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_rootsift)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_rootsift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zs_XcsWtESt"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_rootsift_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_rootsift)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_rootsift_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHcGB_bVtGju"
      },
      "source": [
        "del H_left_rootsift, H_right_rootsift,keypoints_all_left_rootsift, keypoints_all_right_rootsift, descriptors_all_left_rootsift, descriptors_all_right_rootsift, points_all_left_rootsift, points_all_right_rootsift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHX0eCF3uYNd"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_gftt_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_gftt = []\n",
        "descriptors_all_left_gftt = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_gftt.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_gftt.append(keypoints_each)\n",
        "  descriptors_all_left_gftt.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHI5Bx6ruaOB"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_gftt_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_gftt = []\n",
        "descriptors_all_right_gftt = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_gftt.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_gftt.append(keypoints_each)\n",
        "  descriptors_all_right_gftt.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK1djsbWuclu"
      },
      "source": [
        "H_left_gftt = []\n",
        "H_right_gftt = []\n",
        "\n",
        "num_matches_gftt = []\n",
        "num_good_matches_gftt = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_gftt[j:j+2][::-1],points_all_left_gftt[j:j+2][::-1],descriptors_all_left_gftt[j:j+2][::-1],0.8,6)\n",
        "  H_left_gftt.append(H_a)\n",
        "  num_matches_gftt.append(matches)\n",
        "  num_good_matches_gftt.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_gftt[j:j+2][::-1],points_all_right_gftt[j:j+2][::-1],descriptors_all_right_gftt[j:j+2][::-1],0.8,6)\n",
        "  H_right_gftt.append(H_a)\n",
        "  num_matches_gftt.append(matches)\n",
        "  num_good_matches_gftt.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0IgZrDNue5e"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_gftt_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_gftt)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_gftt_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgU2qHCCuh-e"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_gftt_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_gftt)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_gftt_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fTn1UZqukFA"
      },
      "source": [
        "del H_left_gftt, H_right_gftt,keypoints_all_left_gftt, keypoints_all_right_gftt, descriptors_all_left_gftt, descriptors_all_right_gftt, points_all_left_gftt, points_all_right_gftt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ7XLKsErCKc"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_mser_left.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_left_mser = []\n",
        "descriptors_all_left_mser = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_left_mser.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_left_mser.append(keypoints_each)\n",
        "  descriptors_all_left_mser.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4pSqB_frGJd"
      },
      "source": [
        "import pickle\n",
        "Fdb = open('all_feat_mser_right.dat', 'rb')\n",
        "kpts_all = pickle.load(Fdb)\n",
        "Fdb.close()\n",
        "\n",
        "keypoints_all_right_mser = []\n",
        "descriptors_all_right_mser = []\n",
        "\n",
        "for j,kpt_each in enumerate(kpts_all):\n",
        "  keypoints_each = []\n",
        "  descrip_each = []\n",
        "  for k,kpt_img in enumerate(kpt_each):\n",
        "    temp_feature = cv2.KeyPoint(x=kpt_img[0][0],y=kpt_img[0][1],_size=kpt_img[1], _angle=kpt_img[2], \n",
        "                            _response=kpt_img[3], _octave=kpt_img[4], _class_id=kpt_img[5]) \n",
        "    temp_descriptor = kpt_img[6]\n",
        "    keypoints_each.append(temp_feature)\n",
        "    descrip_each.append(temp_descriptor)\n",
        "  points_all_right_mser.append(np.asarray([[p.pt[0], p.pt[1]] for p in keypoints_each]))\n",
        "  keypoints_all_right_mser.append(keypoints_each)\n",
        "  descriptors_all_right_mser.append(descrip_each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hxtobCdrJX_"
      },
      "source": [
        "H_left_mser = []\n",
        "H_right_mser = []\n",
        "images_left_bgr = []\n",
        "images_right_bgr = []\n",
        "num_matches_mser = []\n",
        "num_good_matches_mser = []\n",
        "\n",
        "for j in tqdm(range(len(left_files_path))):\n",
        "  if j==len(left_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_left_bgr[j:j+2][::-1],keypoints_all_left_mser[j:j+2][::-1],points_all_left_mser[j:j+2][::-1],descriptors_all_left_mser[j:j+2][::-1],1,6)\n",
        "  H_left_mser.append(H_a)\n",
        "  num_matches_mser.append(matches)\n",
        "  num_good_matches_mser.append(gd_matches)\n",
        "\n",
        "for j in tqdm(range(len(right_files_path))):\n",
        "  if j==len(right_files_path)-1:\n",
        "    break\n",
        "\n",
        "  H_a,matches,gd_matches = get_Hmatrix(images_right_bgr[j:j+2][::-1],keypoints_all_right_mser[j:j+2][::-1],points_all_right_mser[j:j+2][::-1],descriptors_all_right_mser[j:j+2][::-1],1,6)\n",
        "  H_right_mser.append(H_a)\n",
        "  num_matches_mser.append(matches)\n",
        "  num_good_matches_mser.append(gd_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD3CB5mjdlDM"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_left_mser_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_left_mser)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_left_mser_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJJ8eA5Cdop_"
      },
      "source": [
        "import h5py as h5\n",
        "f=h5.File('drive/MyDrive/H_right_mser_40.h5','w')\n",
        "t0=time.time()\n",
        "f.create_dataset('data',data=H_right_mser)\n",
        "f.close()\n",
        "print('HDF5  w/o comp.:',time.time()-t0,'[s] ... size',os.path.getsize('drive/MyDrive/H_right_mser_40.h5')/1.e6,'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SC0IzJOdrri"
      },
      "source": [
        "del H_left_mser, H_right_mser,keypoints_all_left_mser, keypoints_all_right_mser, descriptors_all_left_mser, descriptors_all_right_mser, points_all_left_mser, points_all_right_mser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InJRk5bugqbJ"
      },
      "source": [
        "len_files = len(left_files_path) + len(right_files_path[1:])\n",
        "num_detectors = 11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnDtNuCSr1HB"
      },
      "source": [
        "d = {'Dataset': [f'{Dataset}']*(num_detectors*len_files), 'Number of Keypoints':  num_kps_akaze + num_kps_brisk + num_kps_daisy + num_kps_freak + num_kps_gftt  + num_kps_mser + num_kps_orb + num_kps_rootsift + num_kps_sift + num_kps_briefstar + num_kps_surf, 'Detector/Descriptor': ['AKAZE']*len_files + ['BRISK']*len_files + ['DAISY+SIFT']*len_files + ['BRISK+FREAK']*len_files + ['GFTT+SIFT']*len_files + ['MSER+SIFT']*len_files + ['ORB']*len_files +['RootSIFT']*len_files +['SIFT']*len_files + ['STAR+BRIEF']*len_files  + ['SURF']*len_files  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzkCYS6ZbNeo"
      },
      "source": [
        "df = pd.DataFrame.from_dict(d, orient='index')\n",
        "df = df.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys2GVsJYXtb4"
      },
      "source": [
        "df_numkey_15 = df\n",
        "df_numkey_15['Number of Keypoints'] = df_numkey_15['Number of Keypoints']/(len_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNRDAu8Rr-dp"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_numkey_15, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"Number of Keypoints\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=6, aspect=2\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset\", \"Number of Keypoints/Descriptors\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"Number of Keypoints Detected for each Detector/Descriptor in Different Aerial Datasets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfLTSdPEsB8W"
      },
      "source": [
        "df_numkey_15.to_csv(f'drive/MyDrive/Num_Kypoints_15_{Dataset}.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJyxL3zFsKrs"
      },
      "source": [
        "d = {'Dataset': [f'{Dataset}']*(num_detectors*(len_files-1)), 'Number of Total Matches':num_matches_akaze + num_matches_brisk + num_matches_daisy + num_matches_freak + num_matches_gftt  + num_matches_mser + num_matches_orb + num_matches_rootsift + num_matches_sift + num_matches_briefstar + num_matches_surf, 'Detector/Descriptor': ['AKAZE']*(len_files-1) + ['BRISK']*(len_files-1) + ['DAISY+SIFT']*(len_files-1) + ['BRISK+FREAK']*(len_files-1) + ['GFTT+SIFT']*(len_files-1) + ['KAZE']*(len_files-1) + ['MSER+SIFT']*(len_files-1) + ['ORB']*(len_files-1) +['RootSIFT']*(len_files-1) +['SIFT']*(len_files-1) + ['STAR+BRIEF']*(len_files-1) +   ['SURF']*(len_files-1) }\n",
        "df_match_15 = pd.DataFrame(data=d)\n",
        "df_match_15['Number of Total Matches'] = df_match_15['Number of Total Matches']/(len_files-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn0NBUjcsTFT"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_match_15, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"Number of Total Matches\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset \", \"Total Number of Matches b/w Consecutive/Overlapping Images\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"Total Number of Matches Detected for each Detector/Descriptor in Different Aerial Datasets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY5eU34Ise2K"
      },
      "source": [
        "df_match_15['Number of Good Matches'] = num_good_matches_akaze + num_good_matches_brisk + num_good_matches_daisy  + num_good_matches_freak + num_good_matches_gftt + num_good_matches_mser + num_good_matches_orb + num_good_matches_rootsift + num_good_matches_sift + num_good_matches_briefstar +  num_good_matches_surf\n",
        "df_match_15['Number of Good Matches'] = df_match_15['Number of Good Matches']/(len_files-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syZCP6m0tIxK"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_match_15, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"Number of Good Matches\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset\", \"Number of Good Matches b/w Consecutive/Overlapping Images\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"Number of Good Matches (Lowe + RANSAC) Detected for each Detector/Descriptor in Different Aerial Datasets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo1VEybZtQpv"
      },
      "source": [
        "df_match_15['Recall Rate of Matches'] = df_match_15['Number of Good Matches']/df_match_15['Number of Total Matches']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SUGUr5ktT8I"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "g = sns.catplot(\n",
        "    data=df_match_15, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"Recall Rate of Matches\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset\", \"Precision of Matches\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"Recall Rate of Matches Detected (Good/Total) for each Detector/Descriptor in Different Aerial Datasets (Higher the Better)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kAux5hatWMn"
      },
      "source": [
        "df_match_15['1 - Precision Rate of Matches'] = (df_match_15['Number of Total Matches'] - df_match_15['Number of Good Matches'])/df_match_15['Number of Total Matches']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD9mY2s3tZf6"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_match_15, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"1 - Precision Rate of Matches\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset (100 Images)\", \"1 - Precision Rate of Matches\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"1 - Precision rate of Matches Detected (False/Total Matches) for each Detector/Descriptor in Different Aerial Datasets (Lower the Better)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDifTR15tb5q"
      },
      "source": [
        "df_match_15['F-Score'] = (2* (1 - df_match_15['1 - Precision Rate of Matches']) * df_match_15['Recall Rate of Matches'])/((1 - df_match_15['1 - Precision Rate of Matches']) + df_match_15['Recall Rate of Matches'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id6Cmlg3te8d"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_match_15, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"F-Score\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset\", \"F-Score\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"F-Score of Matches Detected (2*P*R/P+R) for each Detector/Descriptor in Different Aerial Datasets (Higher the Better)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPXEmH1bth1u"
      },
      "source": [
        "d = {'Dataset': [f'{Dataset}']*(num_detectors), 'Time':  [time_all[0]] + [time_all[1]] + [time_all[2]] + [time_all[3]] + [time_all[4]] + [time_all[5]] + [time_all[6]] + [time_all[7]] + [time_all[8]] + [time_all[9]] + [time_all[10]]  , 'Detector/Descriptor':['AKAZE'] + ['BRISK']*(1) + ['DAISY+SIFT']*(1) + ['BRISK+FREAK']*(1) + ['GFTT+SIFT']*(1) + ['MSER+SIFT']*(1) + ['ORB']*(1) +['RootSIFT']*(1) +['SIFT']*(1) + ['STAR+BRIEF']*(1) + ['SURF']*(1)} \n",
        "df_time_15 = pd.DataFrame(data=d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JAJIlqJtwII"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "\n",
        "# Draw a nested barplot by species and sex\n",
        "g = sns.catplot(\n",
        "    data=df_time_15, kind=\"bar\",\n",
        "    x=\"Dataset\", y=\"Time\", hue=\"Detector/Descriptor\",\n",
        "    ci=\"sd\", palette=\"Spectral\", alpha=.9, height=10, aspect=0.5\n",
        ")\n",
        "g.despine(left=True)\n",
        "g.set_axis_labels(\"Dataset\", \"Time (in sec)\")\n",
        "g.legend.set_title(\"Detector/Descriptor\")\n",
        "g.fig.suptitle(\"Time taken during Feature Extraction by each Detector/Descriptor in Different Aerial Datasets (Lower the Better)\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}